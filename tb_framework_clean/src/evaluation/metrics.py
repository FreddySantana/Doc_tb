"""
M√≥dulo de m√©tricas de avalia√ß√£o (F1-Score, AUC, Precision, Recall)

Autor: Frederico
Institui√ß√£o: Programa de Doutorado
Projeto: Framework Multi-Paradigma para Predi√ß√£o de Abandono de Tratamento de Tuberculose

Data de Cria√ß√£o: 2025-07-01
√öltima Modifica√ß√£o: 2025-09-20

Descri√ß√£o:
    Este m√≥dulo faz parte do framework multi-paradigma desenvolvido para predi√ß√£o
    de abandono de tratamento em pacientes com tuberculose. O framework integra
    t√©cnicas de Machine Learning, Deep Reinforcement Learning, Natural Language
    Processing e Explainable AI.

Licen√ßa: MIT
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Avalia√ß√£o de M√©tricas

M√≥dulo para c√°lculo e an√°lise de m√©tricas de desempenho
conforme descrito na Se√ß√£o 4.6 da tese.
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Optional
import logging
import json

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report,
    roc_curve, precision_recall_curve, average_precision_score
)

from src.utils import setup_logger, load_config

logger = setup_logger(__name__)


class MetricsEvaluator:
    """
    Avaliador de m√©tricas de desempenho.
    
    Calcula m√©tricas padr√£o para classifica√ß√£o bin√°ria:
    - Acur√°cia, Precis√£o, Recall, F1-Score
    - ROC-AUC, Especificidade, Sensibilidade
    - Matriz de confus√£o
    - Curvas ROC e Precision-Recall
    
    Attributes:
        config: Dicion√°rio de configura√ß√µes
        results: Dicion√°rio com todos os resultados
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Inicializa o avaliador.
        
        Args:
            config: Dicion√°rio de configura√ß√µes
        """
        self.config = config
        self.results = {}
        
        self.output_dir = Path('results/evaluation')
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info('MetricsEvaluator inicializado')
    
    def evaluate(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        y_proba: Optional[np.ndarray] = None,
        model_name: str = 'model'
    ) -> Dict[str, Any]:
        """
        Calcula todas as m√©tricas.
        
        Args:
            y_true: Valores verdadeiros
            y_pred: Predi√ß√µes (classes)
            y_proba: Probabilidades (opcional)
            model_name: Nome do modelo
        
        Returns:
            Dicion√°rio com todas as m√©tricas
        """
        logger.info('='*80)
        logger.info(f'AVALIANDO MODELO: {model_name.upper()}')
        logger.info('='*80)
        
        metrics = {}
        
        # M√©tricas b√°sicas
        metrics['accuracy'] = float(accuracy_score(y_true, y_pred))
        metrics['precision'] = float(precision_score(y_true, y_pred, zero_division=0))
        metrics['recall'] = float(recall_score(y_true, y_pred, zero_division=0))
        metrics['f1_score'] = float(f1_score(y_true, y_pred, zero_division=0))
        
        # Matriz de confus√£o
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        metrics['confusion_matrix'] = {
            'true_negatives': int(tn),
            'false_positives': int(fp),
            'false_negatives': int(fn),
            'true_positives': int(tp)
        }
        
        # Especificidade e sensibilidade
        metrics['specificity'] = float(tn / (tn + fp)) if (tn + fp) > 0 else 0.0
        metrics['sensitivity'] = metrics['recall']  # Sensibilidade = Recall
        
        # M√©tricas baseadas em probabilidade (se dispon√≠vel)
        if y_proba is not None:
            metrics['roc_auc'] = float(roc_auc_score(y_true, y_proba))
            metrics['average_precision'] = float(average_precision_score(y_true, y_proba))
            
            # Curvas
            fpr, tpr, roc_thresholds = roc_curve(y_true, y_proba)
            precision, recall, pr_thresholds = precision_recall_curve(y_true, y_proba)
            
            metrics['roc_curve'] = {
                'fpr': fpr.tolist(),
                'tpr': tpr.tolist(),
                'thresholds': roc_thresholds.tolist()
            }
            
            metrics['pr_curve'] = {
                'precision': precision.tolist(),
                'recall': recall.tolist(),
                'thresholds': pr_thresholds.tolist()
            }
        
        # Log das m√©tricas
        logger.info('\nM√©tricas de Desempenho:')
        logger.info(f'  Acur√°cia: {metrics["accuracy"]:.4f}')
        logger.info(f'  Precis√£o: {metrics["precision"]:.4f}')
        logger.info(f'  Recall (Sensibilidade): {metrics["recall"]:.4f}')
        logger.info(f'  F1-Score: {metrics["f1_score"]:.4f}')
        logger.info(f'  Especificidade: {metrics["specificity"]:.4f}')
        
        if 'roc_auc' in metrics:
            logger.info(f'  ROC-AUC: {metrics["roc_auc"]:.4f}')
            logger.info(f'  Average Precision: {metrics["average_precision"]:.4f}')
        
        logger.info('\nMatriz de Confus√£o:')
        logger.info(f'  TN: {tn}  FP: {fp}')
        logger.info(f'  FN: {fn}  TP: {tp}')
        
        # Salvar resultados
        self.results[model_name] = metrics
        
        return metrics
    
    def compare_models(
        self,
        models_metrics: Dict[str, Dict[str, Any]]
    ) -> pd.DataFrame:
        """
        Compara m√©tricas de m√∫ltiplos modelos.
        
        Args:
            models_metrics: Dicion√°rio {nome_modelo: m√©tricas}
        
        Returns:
            DataFrame comparativo
        """
        logger.info('='*80)
        logger.info('COMPARANDO MODELOS')
        logger.info('='*80)
        
        # Extrair m√©tricas principais
        comparison_data = []
        
        for model_name, metrics in models_metrics.items():
            row = {
                'Model': model_name,
                'Accuracy': metrics.get('accuracy', 0),
                'Precision': metrics.get('precision', 0),
                'Recall': metrics.get('recall', 0),
                'F1-Score': metrics.get('f1_score', 0),
                'Specificity': metrics.get('specificity', 0),
                'ROC-AUC': metrics.get('roc_auc', 0)
            }
            comparison_data.append(row)
        
        # Criar DataFrame
        comparison_df = pd.DataFrame(comparison_data)
        comparison_df = comparison_df.sort_values('F1-Score', ascending=False)
        
        # Log
        logger.info('\nCompara√ß√£o de Modelos:')
        logger.info('\n' + comparison_df.to_string(index=False))
        
        # Salvar
        save_path = self.output_dir / 'models_comparison.csv'
        comparison_df.to_csv(save_path, index=False)
        logger.info(f'\nCompara√ß√£o salva em {save_path}')
        
        return comparison_df
    
    def generate_classification_report(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        model_name: str = 'model',
        target_names: List[str] = ['N√£o-Abandono', 'Abandono']
    ) -> str:
        """
        Gera relat√≥rio de classifica√ß√£o detalhado.
        
        Args:
            y_true: Valores verdadeiros
            y_pred: Predi√ß√µes
            model_name: Nome do modelo
            target_names: Nomes das classes
        
        Returns:
            String com relat√≥rio
        """
        logger.info('='*80)
        logger.info(f'RELAT√ìRIO DE CLASSIFICA√á√ÉO: {model_name.upper()}')
        logger.info('='*80)
        
        report = classification_report(
            y_true, y_pred,
            target_names=target_names,
            digits=4
        )
        
        logger.info('\n' + report)
        
        # Salvar
        save_path = self.output_dir / f'{model_name}_classification_report.txt'
        with open(save_path, 'w') as f:
            f.write(f'Classification Report: {model_name}\n')
            f.write('='*80 + '\n\n')
            f.write(report)
        
        logger.info(f'Relat√≥rio salvo em {save_path}')
        
        return report
    
    def calculate_clinical_metrics(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        model_name: str = 'model'
    ) -> Dict[str, float]:
        """
        Calcula m√©tricas cl√≠nicas espec√≠ficas para TB.
        
        Args:
            y_true: Valores verdadeiros
            y_pred: Predi√ß√µes
            model_name: Nome do modelo
        
        Returns:
            Dicion√°rio com m√©tricas cl√≠nicas
        """
        logger.info('='*80)
        logger.info(f'M√âTRICAS CL√çNICAS: {model_name.upper()}')
        logger.info('='*80)
        
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        # M√©tricas cl√≠nicas
        clinical_metrics = {}
        
        # Positive Predictive Value (PPV) = Precis√£o
        clinical_metrics['ppv'] = float(tp / (tp + fp)) if (tp + fp) > 0 else 0.0
        
        # Negative Predictive Value (NPV)
        clinical_metrics['npv'] = float(tn / (tn + fn)) if (tn + fn) > 0 else 0.0
        
        # Likelihood Ratio Positive (LR+)
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        clinical_metrics['lr_positive'] = float(sensitivity / (1 - specificity)) if specificity < 1 else float('inf')
        
        # Likelihood Ratio Negative (LR-)
        clinical_metrics['lr_negative'] = float((1 - sensitivity) / specificity) if specificity > 0 else 0.0
        
        # Number Needed to Screen (NNS) - aproxima√ß√£o
        prevalence = (tp + fn) / (tn + fp + fn + tp)
        clinical_metrics['prevalence'] = float(prevalence)
        clinical_metrics['nns'] = float(1 / (sensitivity * prevalence)) if (sensitivity * prevalence) > 0 else float('inf')
        
        # Log
        logger.info('\nM√©tricas Cl√≠nicas:')
        logger.info(f'  PPV (Valor Preditivo Positivo): {clinical_metrics["ppv"]:.4f}')
        logger.info(f'  NPV (Valor Preditivo Negativo): {clinical_metrics["npv"]:.4f}')
        logger.info(f'  LR+ (Raz√£o de Verossimilhan√ßa Positiva): {clinical_metrics["lr_positive"]:.4f}')
        logger.info(f'  LR- (Raz√£o de Verossimilhan√ßa Negativa): {clinical_metrics["lr_negative"]:.4f}')
        logger.info(f'  Preval√™ncia: {clinical_metrics["prevalence"]:.4f}')
        logger.info(f'  NNS (N√∫mero Necess√°rio para Rastrear): {clinical_metrics["nns"]:.2f}')
        
        return clinical_metrics
    
    def save_all_results(self, filename: str = 'evaluation_results.json') -> None:
        """
        Salva todos os resultados.
        
        Args:
            filename: Nome do arquivo
        """
        save_path = self.output_dir / filename
        
        with open(save_path, 'w') as f:
            json.dump(self.results, f, indent=2)
        
        logger.info(f'Todos os resultados salvos em {save_path}')


def main():
    """Fun√ß√£o principal para execu√ß√£o standalone"""
    from src.utils import load_data, load_model
    
    logger.info('Iniciando avalia√ß√£o de m√©tricas')
    
    # Carregar configura√ß√£o
    config = load_config()
    evaluator = MetricsEvaluator(config)
    
    # Carregar dados e modelo
    logger.info('Carregando dados...')
    test_df = load_data('data/processed/test.csv')
    
    target_col = config['target']['column_name']
    X_test = test_df.drop(target_col, axis=1)
    y_test = test_df[target_col].values
    
    logger.info('Carregando modelo...')
    model = load_model('results/ml_models/xgboost/xgboost_model.pkl')
    
    # Predi√ß√µes
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]
    
    # Avaliar
    metrics = evaluator.evaluate(y_test, y_pred, y_proba, model_name='XGBoost')
    
    # Relat√≥rio de classifica√ß√£o
    evaluator.generate_classification_report(y_test, y_pred, model_name='XGBoost')
    
    # M√©tricas cl√≠nicas
    clinical_metrics = evaluator.calculate_clinical_metrics(y_test, y_pred, model_name='XGBoost')
    
    # Salvar
    evaluator.save_all_results()
    
    print('\n' + '='*80)
    print('‚úÖ AVALIA√á√ÉO DE M√âTRICAS CONCLU√çDA COM SUCESSO!')
    print('='*80)
    print(f'\nüìÅ Resultados salvos em: {evaluator.output_dir}')
    print('='*80)


if __name__ == '__main__':
    main()
