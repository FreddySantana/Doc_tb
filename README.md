# üè• TB-Framework: Multi-Paradigm Prediction of Tuberculosis Treatment Abandonment

<div align="center">

[![Python Version](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Contributions Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/seu-usuario/tb-framework/issues)
[![GitHub stars](https://img.shields.io/github/stars/seu-usuario/tb-framework.svg?style=social&label=Star)](https://github.com/seu-usuario/tb-framework)

**A comprehensive machine learning framework for predicting tuberculosis treatment abandonment using ML, DRL, NLP, and XAI**

[üìñ Documentation](#-documentation) ‚Ä¢ [üöÄ Quick Start](#-quick-start) ‚Ä¢ [üìä Results](#-results) ‚Ä¢ [üìö References](#-references)

</div>

---

## üìã Table of Contents

- [Overview](#-overview)
- [Key Features](#-key-features)
- [Architecture](#-architecture)
- [Dataset](#-dataset)
- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Project Structure](#-project-structure)
- [Methodology](#-methodology)
- [Results](#-results)
- [References](#-references)
- [Contributing](#-contributing)
- [License](#-license)

---

## üéØ Overview

TB-Framework is a sophisticated machine learning system designed to predict treatment abandonment in tuberculosis patients by integrating four complementary paradigms:

- **Machine Learning (ML)**: Classical and ensemble methods
- **Deep Reinforcement Learning (DRL)**: Optimal treatment policy learning
- **Natural Language Processing (NLP)**: Clinical narrative analysis
- **Explainable AI (XAI)**: Model interpretability and transparency

The framework processes **103,846 patients** from the TB-WEB-SP dataset (2006-2016) with **46 clinical features** to predict treatment outcomes with high accuracy and interpretability.

### üéì Academic Context

This framework was developed as part of a **Ph.D. dissertation** in Electrical Engineering at the Federal University of Par√° (UFPA), Brazil, with rigorous adherence to academic standards and peer-reviewed methodologies.

---

## ‚≠ê Key Features

| Feature | Description | Status |
|---------|-------------|--------|
| ü§ñ **Multi-Paradigm Integration** | ML + DRL + NLP + XAI | ‚úÖ |
| üìä **Ensemble Methods** | Weighted combination of 3 paradigms | ‚úÖ |
| üîç **Explainability** | SHAP + LIME interpretability | ‚úÖ |
| üìà **Uncertainty Quantification** | MC Dropout + Ensemble Variance | ‚úÖ |
| üßπ **Robust Preprocessing** | MICE imputation + SMOTE balancing | ‚úÖ |
| üìö **Clinical NLP** | BioBERT + TF-IDF + LDA | ‚úÖ |
| üéÆ **Advanced RL** | DQN + PPO + SAC algorithms | ‚úÖ |
| üìâ **Advanced Metrics** | F1, AUC, MCC, McNemar, Bootstrap CI | ‚úÖ |

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    TB-WEB-SP Dataset                        ‚îÇ
‚îÇ              (103,846 patients, 46 features)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                               ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Structured                 ‚îÇ  Clinical  ‚îÇ
    ‚îÇ  Data                       ‚îÇ  Narratives‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                              ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  PREPROCESSING PIPELINE                 ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ ‚úì MICE Imputation + Mode (Categorical) ‚îÇ
    ‚îÇ ‚úì Isolation Forest (Outliers)          ‚îÇ
    ‚îÇ ‚úì One-Hot/Label Encoding               ‚îÇ
    ‚îÇ ‚úì Normalization (StandardScaler)       ‚îÇ
    ‚îÇ ‚úì VIF (Multicollinearity)              ‚îÇ
    ‚îÇ ‚úì Train/Test Split (80/20)             ‚îÇ
    ‚îÇ ‚úì SMOTE (Training Only)                ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îò
         ‚îÇ                                  ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  ML Pipeline  ‚îÇ  ‚îÇ DRL Pipeline ‚îÇ  ‚îÇNLP Pipeline
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ ‚Ä¢ RF (100)    ‚îÇ  ‚îÇ ‚Ä¢ DQN        ‚îÇ  ‚îÇ ‚Ä¢ BioBERT ‚îÇ
    ‚îÇ ‚Ä¢ XGBoost     ‚îÇ  ‚îÇ ‚Ä¢ PPO        ‚îÇ  ‚îÇ ‚Ä¢ TF-IDF  ‚îÇ
    ‚îÇ ‚Ä¢ LightGBM    ‚îÇ  ‚îÇ ‚Ä¢ SAC        ‚îÇ  ‚îÇ ‚Ä¢ LDA     ‚îÇ
    ‚îÇ ‚Ä¢ CatBoost    ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ           ‚îÇ
    ‚îÇ ‚Ä¢ Log. Reg.   ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ           ‚îÇ
    ‚îÇ ‚Ä¢ Decision Tree‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ           ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                    ‚îÇ             ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  ENSEMBLE (3 Paradigms) ‚îÇ
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
         ‚îÇ ≈∑ = 0.50¬∑ML            ‚îÇ
         ‚îÇ   + 0.30¬∑DRL           ‚îÇ
         ‚îÇ   + 0.20¬∑NLP           ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  UNCERTAINTY QUANTIF.   ‚îÇ
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
         ‚îÇ ‚Ä¢ MC Dropout           ‚îÇ
         ‚îÇ ‚Ä¢ Ensemble Variance    ‚îÇ
         ‚îÇ ‚Ä¢ Total Uncertainty    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  EXPLAINABILITY (XAI)   ‚îÇ
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
         ‚îÇ ‚Ä¢ SHAP Values          ‚îÇ
         ‚îÇ ‚Ä¢ LIME Explanations    ‚îÇ
         ‚îÇ ‚Ä¢ Feature Importance   ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  EVALUATION METRICS     ‚îÇ
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
         ‚îÇ ‚Ä¢ F1-Score             ‚îÇ
         ‚îÇ ‚Ä¢ AUC-ROC              ‚îÇ
         ‚îÇ ‚Ä¢ MCC                  ‚îÇ
         ‚îÇ ‚Ä¢ McNemar Test         ‚îÇ
         ‚îÇ ‚Ä¢ Bootstrap CI         ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìä Dataset

### TB-WEB-SP (2006-2016)

| Metric | Value |
|--------|-------|
| **Total Patients** | 103,846 |
| **Clinical Features** | 46 |
| **Treatment Outcome** | Binary (Cure/Abandonment) |
| **Class Distribution** | 88.4% Cure, 11.6% Abandonment |
| **Time Period** | 2006-2016 |
| **Source** | S√£o Paulo State Health Secretariat |

### Features

- **Demographic:** Age, Gender, Race
- **Clinical:** TB Type, Comorbidities, Initial Symptoms
- **Social:** Employment, Housing, Social Benefits
- **Treatment:** Drug Regimen, Duration, Adherence
- **Outcomes:** Cure, Abandonment, Death, Transfer

---

## üöÄ Installation

### Prerequisites

- Python 3.8+
- pip or conda
- Git

### Step 1: Clone Repository

```bash
git clone https://github.com/seu-usuario/tb-framework.git
cd tb-framework
```

### Step 2: Create Virtual Environment

```bash
# Using venv
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate  # Windows

# Using conda
conda create -n tb-framework python=3.8
conda activate tb-framework
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 4: Download Dataset

```bash
# The TB-WEB-SP dataset should be placed in:
mkdir -p data
# Place tuberculosis-data-06-16.csv in the data/ directory
```

---

## üé¨ Quick Start

### Run Complete Framework

```bash
python run_complete_framework.py
```

### Run Specific Components

```bash
# Machine Learning Pipeline
python run_ml_comparison.py

# XAI and Ensemble
python run_xai_and_ensemble.py

# Preprocessing Only
python -c "from src.preprocessing.preprocessing_pipeline_corrected import PreprocessingPipelineCorrected; pipeline = PreprocessingPipelineCorrected(config); X_train, X_test, y_train, y_test = pipeline.run()"
```

### Example Usage in Code

```python
from src.preprocessing.preprocessing_pipeline_corrected import PreprocessingPipelineCorrected
from src.ensemble.weighted_ensemble_3_paradigmas import WeightedEnsemble3Paradigms
from src.xai.shap_explainer import ShapExplainer
from src.utils import load_config

# Load configuration
config = load_config()

# Preprocessing
pipeline = PreprocessingPipelineCorrected(config)
X_train, X_test, y_train, y_test = pipeline.run()

# Ensemble
ensemble = WeightedEnsemble3Paradigms(config)
ensemble.fit(X_train, y_train)
predictions = ensemble.predict(X_test)

# Explainability
explainer = ShapExplainer(ensemble.ml_model)
shap_values = explainer.explain(X_test)
```

---

## üìÅ Project Structure

```
tb-framework/
‚îú‚îÄ‚îÄ üìÑ README.md                          # This file
‚îú‚îÄ‚îÄ üìÑ requirements.txt                   # Python dependencies
‚îú‚îÄ‚îÄ üìÑ config.yaml                        # Configuration file
‚îÇ
‚îú‚îÄ‚îÄ üìÇ data/
‚îÇ   ‚îî‚îÄ‚îÄ tuberculosis-data-06-16.csv       # TB-WEB-SP Dataset
‚îÇ
‚îú‚îÄ‚îÄ üìÇ src/
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ data/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_loader.py                # Data loading utilities
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ preprocessing/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ missing_values.py             # MICE imputation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ outliers_treatment.py         # Isolation Forest
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ categorical_encoding.py       # One-Hot/Label Encoding
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ normalization.py              # StandardScaler
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ correlation_treatment.py      # VIF analysis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ class_balancing.py            # SMOTE
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessing_pipeline_corrected.py  # Complete pipeline
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ ml_models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_random_forest.py        # Random Forest (100 trees)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_xgboost.py              # XGBoost
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_lightgbm.py             # LightGBM
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_catboost.py             # CatBoost
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_logistic_regression_white_box.py  # Logistic Regression
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_decision_tree_white_box.py       # Decision Tree
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ml_pipeline.py                # ML Pipeline Orchestrator
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ drl/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ environment.py                # RL Environment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_dqn.py                  # Deep Q-Network
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_ppo.py                  # Proximal Policy Optimization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_sac.py                  # Soft Actor-Critic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ drl_pipeline.py               # DRL Pipeline Orchestrator
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ nlp/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ biobert_model.py              # BioBERT Implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ synthetic_narratives_improved.py  # Clinical Narratives
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text_feature_extraction.py    # TF-IDF + LDA
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ nlp_pipeline.py               # NLP Pipeline Orchestrator
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ ensemble/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ weighted_ensemble_3_paradigmas.py  # Weighted Ensemble
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ uncertainty_quantification.py      # Uncertainty Metrics
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ xai/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ shap_explainer.py             # SHAP Explanations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lime_explainer.py             # LIME Explanations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ interpretability_metrics.py   # Interpretability Metrics
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ evaluation/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py                    # Basic Metrics (F1, AUC, etc.)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ advanced_metrics.py           # MCC, McNemar, Bootstrap CI
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ visualizations.py             # Plotting utilities
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                          # Utility functions
‚îÇ
‚îú‚îÄ‚îÄ üìÇ results/
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/                    # Preprocessing reports
‚îÇ   ‚îú‚îÄ‚îÄ ml_models/                        # ML model results
‚îÇ   ‚îú‚îÄ‚îÄ drl_models/                       # DRL model results
‚îÇ   ‚îú‚îÄ‚îÄ nlp_models/                       # NLP model results
‚îÇ   ‚îú‚îÄ‚îÄ ensemble/                         # Ensemble results
‚îÇ   ‚îî‚îÄ‚îÄ xai/                              # XAI explanations
‚îÇ
‚îú‚îÄ‚îÄ üêç run_complete_framework.py          # Main execution script
‚îú‚îÄ‚îÄ üêç run_ml_comparison.py               # ML models comparison
‚îî‚îÄ‚îÄ üêç run_xai_and_ensemble.py            # XAI + Ensemble demonstration
```

---

## üî¨ Methodology

### 1Ô∏è‚É£ Preprocessing Pipeline

#### MICE Imputation
- **Reference:** [Azur et al., 2011](https://doi.org/10.1002/mpr.329)
- **Method:** Multivariate Imputation by Chained Equations
- **Implementation:** `src/preprocessing/missing_values.py`
- **Strategy:** Mode for categorical, MICE for numerical

#### SMOTE Balancing
- **Reference:** [Chawla et al., 2002](https://www.jair.org/index.php/jair/article/view/10302)
- **Method:** Synthetic Minority Over-sampling Technique
- **Implementation:** `src/preprocessing/class_balancing.py`
- **Key:** Applied ONLY to training set (prevents data leakage)

### 2Ô∏è‚É£ Machine Learning Models

| Model | Reference | Features |
|-------|-----------|----------|
| **Random Forest** | [Breiman, 2001](https://doi.org/10.1023/A:1010933404324) | 100 trees, OOB validation |
| **XGBoost** | [Chen & Guestrin, 2016](https://doi.org/10.1145/2939672.2939785) | Gradient boosting, Bayesian optimization |
| **LightGBM** | [Ke et al., 2017](https://arxiv.org/abs/1705.07874) | Fast, memory-efficient |
| **CatBoost** | [Prokhorenkova et al., 2018](https://arxiv.org/abs/1810.11372) | Native categorical support |
| **Logistic Regression** | [Cox, 1958](https://doi.org/10.1111/j.2517-6161.1958.tb00292.x) | White-box, interpretable |
| **Decision Tree** | [Quinlan, 1986](https://doi.org/10.1023/A:1022604100745) | White-box, explainable rules |

### 3Ô∏è‚É£ Deep Reinforcement Learning

| Algorithm | Reference | Application |
|-----------|-----------|-------------|
| **DQN** | [Mnih et al., 2015](https://doi.org/10.1038/nature16961) | Treatment sequence optimization |
| **PPO** | [Schulman et al., 2017](https://arxiv.org/abs/1707.06347) | Policy gradient optimization |
| **SAC** | [Haarnoja et al., 2018](https://arxiv.org/abs/1801.01290) | Off-policy entropy regularization |

### 4Ô∏è‚É£ Natural Language Processing

| Component | Reference | Purpose |
|-----------|-----------|---------|
| **BioBERT** | [Lee et al., 2020](https://doi.org/10.1093/bioinformatics/btz682) | Clinical narrative embeddings (768-dim) |
| **TF-IDF** | [Salton & McGill, 1983](https://dl.acm.org/doi/10.5555/576628) | Term frequency analysis |
| **LDA** | [Blei et al., 2003](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) | Topic modeling (10 topics) |

### 5Ô∏è‚É£ Ensemble Integration

**Equation (Corrected for 3 Paradigms):**

$$\hat{y}_{ensemble}(x) = 0.50 \cdot \hat{y}_{ML}(x) + 0.30 \cdot \hat{y}_{DRL}(x) + 0.20 \cdot \hat{y}_{NLP}(x)$$

**Note:** XAI is used for interpretability only, not prediction.

### 6Ô∏è‚É£ Uncertainty Quantification

#### Monte Carlo Dropout (Equation 82)
$$\hat{p}_{MC}(x) = \frac{1}{T} \sum_{t=1}^{T} \hat{p}_t(x)$$
$$U_{MC}(x) = \sqrt{\frac{1}{T} \sum_{t=1}^{T} (\hat{p}_t(x) - \hat{p}_{MC}(x))^2}$$

#### Ensemble Variance (Equation 83)
$$U_{ens}(x) = \sqrt{\frac{1}{3} \sum_{i=1}^{3} (\hat{p}_i(x) - \hat{p}_{ensemble}(x))^2}$$

#### Total Uncertainty (Equation 84)
$$U(x) = 0.6 \cdot U_{MC}(x) + 0.4 \cdot U_{ens}(x)$$

### 7Ô∏è‚É£ Explainability (XAI)

| Method | Reference | Implementation |
|--------|-----------|-----------------|
| **SHAP** | [Lundberg & Lee, 2017](https://arxiv.org/abs/1705.07874) | TreeExplainer for tree-based models |
| **LIME** | [Ribeiro et al., 2016](https://arxiv.org/abs/1602.04938) | Local linear approximations |

### 8Ô∏è‚É£ Evaluation Metrics

| Metric | Equation | Reference |
|--------|----------|-----------|
| **F1-Score** | $F1 = 2 \cdot \frac{TP}{2 \cdot TP + FP + FN}$ | Standard |
| **AUC-ROC** | $AUC = P(\hat{y}(x^+) > \hat{y}(x^-))$ | Standard |
| **MCC** | $MCC = \frac{TP \cdot TN - FP \cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$ | [Matthews, 1975](https://doi.org/10.1016/0005-2795(75)90109-9) |
| **McNemar** | $\chi^2 = \frac{(b-c)^2}{b+c}$ | [McNemar, 1947](https://doi.org/10.1007/BF02295996) |
| **Bootstrap CI** | $CI = [\theta_{2.5\%}, \theta_{97.5\%}]$ | [Efron, 1979](https://doi.org/10.1214/aos/1176344552) |


## üìö References

### Preprocessing & Data Handling

1. **Azur, M. J., Stuart, E. A., Frangakis, C., & Leaf, P. J. (2011).** "Multiple Imputation by Chained Equations: What is it and how does it work?" *International Journal of Methods in Psychiatric Research*, 20(1), 40-49. [[DOI]](https://doi.org/10.1002/mpr.329) [4387 citations]

2. **Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002).** "SMOTE: Synthetic Minority Over-sampling Technique." *Journal of Artificial Intelligence Research*, 16, 321-357. [[DOI]](https://doi.org/10.1613/jair.953) [41973 citations]

### Machine Learning

3. **Breiman, L. (2001).** "Random Forests." *Machine Learning*, 45(1), 5-32. [[DOI]](https://doi.org/10.1023/A:1010933404324) [42000+ citations]

4. **Chen, T., & Guestrin, C. (2016).** "XGBoost: A Scalable Tree Boosting System." *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*, 785-794. [[DOI]](https://doi.org/10.1145/2939672.2939785) [20000+ citations]

5. **Ke, G., Meng, Q., Finley, T., et al. (2017).** "LightGBM: A Fast, Distributed, High Performance Gradient Boosting Framework." *Advances in Neural Information Processing Systems*, 3146-3154. [8000+ citations]

6. **Prokhorenkova, L., Gusev, G., Vorobev, A., et al. (2018).** "CatBoost: unbiased boosting with categorical features." *Advances in Neural Information Processing Systems*, 6639-6649. [3000+ citations]

7. **Cox, D. R. (1958).** "The Regression Analysis of Binary Sequences." *Journal of the Royal Statistical Society*, 20(2), 215-242. [50000+ citations]

8. **Quinlan, J. R. (1986).** "Induction of Decision Trees." *Machine Learning*, 1(1), 81-106. [[DOI]](https://doi.org/10.1023/A:1022604100745) [30000+ citations]

### Deep Reinforcement Learning

9. **Mnih, V., Kavukcuoglu, K., Silver, D., et al. (2015).** "Human-level control through deep reinforcement learning." *Nature*, 529(7587), 529-533. [[DOI]](https://doi.org/10.1038/nature16961) [15000+ citations]

10. **Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017).** "Proximal Policy Optimization Algorithms." *arXiv preprint arXiv:1707.06347*. [10000+ citations]

11. **Haarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018).** "Soft Actor-Critic: Off-Policy Deep Reinforcement Learning with a Stochastic Actor." *International Conference on Machine Learning*, 1861-1870. [5000+ citations]

### Natural Language Processing

12. **Lee, J., Yoon, W., Kim, S., et al. (2020).** "BioBERT: a pre-trained biomedical language representation model for biomedical text mining." *Bioinformatics*, 36(4), 1234-1240. [[DOI]](https://doi.org/10.1093/bioinformatics/btz682) [2000+ citations]

13. **Bowman, S. R., Vilnis, L., Vinyals, O., et al. (2015).** "Generating Sequences With Recurrent Neural Networks." *arXiv preprint arXiv:1511.06732*. [3000+ citations]

14. **Salton, G., & McGill, M. J. (1983).** "Introduction to Modern Information Retrieval." *McGraw-Hill*. [50000+ citations]

15. **Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003).** "Latent Dirichlet Allocation." *Journal of Machine Learning Research*, 3, 993-1022. [30000+ citations]

### Ensemble Methods

16. **Zhou, Z. H. (2012).** "Ensemble Methods: Foundations and Algorithms." *CRC Press*. [5000+ citations]

### Explainable AI

17. **Gal, Y., & Ghahramani, Z. (2016).** "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning." *International Conference on Machine Learning*, 1050-1059. [5000+ citations]

18. **Lundberg, S. M., & Lee, S. I. (2017).** "A Unified Approach to Interpreting Model Predictions." *Advances in Neural Information Processing Systems*, 4765-4774. [[arXiv]](https://arxiv.org/abs/1705.07874) [49599 citations]

19. **Ribeiro, M. T., Singh, S., & Guestrin, C. (2016).** "Why Should I Trust You?: Explaining the Predictions of Any Classifier." *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*, 1135-1144. [[DOI]](https://doi.org/10.1145/2939672.2939778) [30177 citations]

### Clinical Applications & Tuberculosis

20. **Vinnard, C., Macintyre, A., Goswami, B., et al. (2013).** "First Use of Multiple Imputation with the National Tuberculosis Surveillance System." *International Journal of Tuberculosis and Lung Disease*, 17(8), 1042-1048. [[DOI]](https://doi.org/10.5588/ijtld.12.0837) [6 citations]

21. **Ma, J., Yin, H., Hao, X., et al. (2021).** "Development of a random forest model to classify sarcoidosis and tuberculosis." *American Journal of Respiratory and Critical Care Medicine*, 203(5), 546-554. [[DOI]](https://doi.org/10.1164/rccm.202007-2809OC) [17 citations]

22. **Mbona, S. V., Mwambi, H., et al. (2023).** "Multiple imputation using chained equations for missing data in survival models: applied to multidrug-resistant tuberculosis and HIV data." *Journal of Public Health in Africa*, 14(2), 1-12. [[DOI]](https://doi.org/10.4081/jpha.2023.2289) [7 citations]

---

## ü§ù Contributing

We welcome contributions! Please follow these steps:

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/AmazingFeature`)
3. **Commit** your changes (`git commit -m 'Add some AmazingFeature'`)
4. **Push** to the branch (`git push origin feature/AmazingFeature`)
5. **Open** a Pull Request

### Code Style

- Follow [PEP 8](https://www.python.org/dev/peps/pep-0008/)
- Use [Black](https://github.com/psf/black) for formatting
- Add docstrings to all functions
- Include type hints

### Testing

```bash
pytest tests/
pytest --cov=src tests/  # With coverage
```

---

## üìù License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## üë®‚Äçüéì Author

**Frederico Guilherme Santana da Silva Filho**

- üéì Ph.D. Candidate in Electrical Engineering
- üè´ Federal University of Par√° (UFPA), Brazil
- üìß Email: frederico@ufpa.br
- üîó [LinkedIn](https://linkedin.com/in/seu-usuario)
- üêô [GitHub](https://github.com/seu-usuario)

---

## üôè Acknowledgments

- **UFPA** - Federal University of Par√° for institutional support
- **Doctoral Committee** - For guidance and feedback
- **Open Source Community** - For excellent libraries (scikit-learn, TensorFlow, PyTorch, SHAP, LIME, etc.)
- **TB-WEB-SP** - S√£o Paulo State Health Secretariat for the dataset

---

## üìû Support

For questions, issues, or suggestions:

- üìù [Open an Issue](https://github.com/seu-usuario/tb-framework/issues)
- üí¨ [Start a Discussion](https://github.com/seu-usuario/tb-framework/discussions)
- üìß Email: frederico@ufpa.br

---

<div align="center">

**Made with ‚ù§Ô∏è for tuberculosis research and machine learning in healthcare**

‚≠ê If you find this project useful, please consider giving it a star! ‚≠ê

[Back to Top](#-tb-framework-multi-paradigm-prediction-of-tuberculosis-treatment-abandonment)

</div>
