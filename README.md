VisÃ£o Geral
Este framework implementa uma abordagem multi-paradigma para prediÃ§Ã£o de abandono de tratamento em pacientes com tuberculose, integrando:

â€¢	Machine Learning (ML): Modelos clÃ¡ssicos e ensemble
â€¢	Deep Reinforcement Learning (DRL): OtimizaÃ§Ã£o de polÃ­ticas de tratamento
â€¢	Natural Language Processing (NLP): AnÃ¡lise de narrativas clÃ­nicas
â€¢	Explainable AI (XAI): Interpretabilidade das prediÃ§Ãµes

Dados
â€¢	Dataset: TB-WEB-SP (2006-2016)
â€¢	Pacientes: 103.846
â€¢	Features: 46 variÃ¡veis clÃ­nicas
â€¢	Target: Abandono (11.6%) vs Cura (88.4%)



 Arquitetura do Framework
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TB-WEB-SP Dataset                        â”‚
â”‚              (103.846 pacientes, 46 features)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                               â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚  Dados   â”‚                  â”‚ Narrativas â”‚
    â”‚Estruturados                 â”‚  ClÃ­nicas  â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚                              â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚  PRÃ‰-PROCESSAMENTO                      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ 1. Valores Ausentes (MICE + Moda)      â”‚
    â”‚ 2. Outliers (Isolation Forest)         â”‚
    â”‚ 3. Encoding CategÃ³rico (One-Hot/Label) â”‚
    â”‚ 4. NormalizaÃ§Ã£o                        â”‚
    â”‚ 5. CorrelaÃ§Ã£o (VIF)                    â”‚
    â”‚ 6. Split Treino/Teste                  â”‚
    â”‚ 7. SMOTE (apenas treino)               â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”˜
         â”‚                                  â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ML Pipeline  â”‚  â”‚ DRL Pipeline â”‚  â”‚NLP Pipeline
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ â€¢ RF          â”‚  â”‚ â€¢ DQN        â”‚  â”‚ â€¢ BioBERT â”‚
    â”‚ â€¢ XGBoost     â”‚  â”‚ â€¢ PPO        â”‚  â”‚ â€¢ TF-IDF  â”‚
    â”‚ â€¢ LightGBM    â”‚  â”‚ â€¢ SAC        â”‚  â”‚ â€¢ LDA     â”‚
    â”‚ â€¢ CatBoost    â”‚  â”‚              â”‚  â”‚           â”‚
    â”‚ â€¢ Log. Reg.   â”‚  â”‚              â”‚  â”‚           â”‚
    â”‚ â€¢ Ãrvore Dec. â”‚  â”‚              â”‚  â”‚           â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â”‚             â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  ENSEMBLE (3 parad.)â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚ Pesos: ML=0.50     â”‚
         â”‚        DRL=0.30    â”‚
         â”‚        NLP=0.20    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  QUANTIFICAÃ‡ÃƒO DE   â”‚
         â”‚  INCERTEZA          â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚ â€¢ MC Dropout       â”‚
         â”‚ â€¢ VariÃ¢ncia Ens.   â”‚
         â”‚ â€¢ Incerteza Total  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  XAI                â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚ â€¢ SHAP             â”‚
         â”‚ â€¢ LIME             â”‚
         â”‚ â€¢ Interpretabilidade
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  AVALIAÃ‡ÃƒO          â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚ â€¢ F1-Score         â”‚
         â”‚ â€¢ AUC-ROC          â”‚
         â”‚ â€¢ MCC              â”‚
         â”‚ â€¢ McNemar          â”‚
         â”‚ â€¢ Bootstrap CI     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜



Metodologia por Etapa
1. PRÃ‰-PROCESSAMENTO
1.1 Tratamento de Valores Ausentes (MICE)
ReferÃªncia: [Azur et al., 2011][1] - "Multiple Imputation by Chained Equations: What is it and how does it work?"

ImplementaÃ§Ã£o:
â€¢	Arquivo: src/preprocessing/missing_values.py
â€¢	MÃ©todo: MICE (Multivariate Imputation by Chained Equations)
â€¢	EstratÃ©gia:
â—¦	Passo 1: ImputaÃ§Ã£o por moda para variÃ¡veis categÃ³ricas
â—¦	Passo 2: MICE para variÃ¡veis numÃ©ricas (max_iter=10)

CÃ³digo:
from src.preprocessing.missing_values import MissingValuesHandler
 
handler = MissingValuesHandler(config)
df_imputed = handler.fit_transform(df, strategy='mice')

Justificativa: MICE Ã© recomendado para dados clÃ­nicos com padrÃµes complexos de ausÃªncia, preservando relaÃ§Ãµes multivariadas.



1.2 Tratamento de Outliers
ReferÃªncia: Liu et al., 2008 - "Isolation Forest"

ImplementaÃ§Ã£o:
â€¢	Arquivo: src/preprocessing/outliers_treatment.py
â€¢	MÃ©todo: Isolation Forest
â€¢	Anomaly Score: -0.5 (threshold)

Justificativa: Isolation Forest Ã© nÃ£o-paramÃ©trico e eficiente para detecÃ§Ã£o de anomalias em dados clÃ­nicos.



1.3 Encoding de VariÃ¡veis CategÃ³ricas
ReferÃªncia: Potdar et al., 2017 - "A Comparative Study of Categorical Variable Encoding Techniques"

ImplementaÃ§Ã£o:
â€¢	Arquivo: src/preprocessing/categorical_encoding.py
â€¢	EstratÃ©gia Mista:
â—¦	One-Hot Encoding para â‰¤5 categorias
â—¦	Label Encoding para >5 categorias

CÃ³digo:
from src.preprocessing.categorical_encoding import CategoricalEncoder
 
encoder = CategoricalEncoder(config)
df_encoded = encoder.fit_transform(df, strategy='mixed')



1.4 Balanceamento de Classes (SMOTE)
ReferÃªncia: [Chawla et al., 2002][2] - "SMOTE: Synthetic Minority Over-sampling Technique"

ImplementaÃ§Ã£o:
â€¢	Arquivo: src/preprocessing/class_balancing.py
â€¢	MÃ©todo: SMOTE (k_neighbors=5)
â€¢	AplicaÃ§Ã£o: APENAS no conjunto de treino
â€¢	Split: 80% treino, 20% teste (estratificado)

CÃ³digo:
from src.preprocessing.class_balancing import ClassBalancer
 
balancer = ClassBalancer(config)
X_train_bal, X_test, y_train_bal, y_test = balancer.fit_transform(df)

Justificativa: 
â€¢	Evita data leakage (split antes de SMOTE)
â€¢	Teste reflete distribuiÃ§Ã£o real
â€¢	Balanceamento apenas no treino



2. MACHINE LEARNING
2.1 Random Forest
ReferÃªncia: [Breiman, 2001][3] - "Random Forests"

ImplementaÃ§Ã£o:
â€¢	Arquivo: src/ml_models/train_random_forest.py
â€¢	ConfiguraÃ§Ã£o:
â—¦	n_estimators: 100
â—¦	max_depth: 15
â—¦	min_samples_split: 10
â—¦	OOB Score: ValidaÃ§Ã£o interna

EquaÃ§Ã£o (Algoritmo 4 - Tese):
Å·_RF(x) = (1/B) Î£ T_b(x)



2.2 XGBoost, LightGBM, CatBoost
ReferÃªncias:
â€¢	XGBoost: [Chen & Guestrin, 2016][4] - "XGBoost: A Scalable Tree Boosting System"
â€¢	LightGBM: [Ke et al., 2017][5] - "LightGBM: A Fast, Distributed, High Performance Gradient Boosting"
â€¢	CatBoost: [Prokhorenkova et al., 2018][6] - "CatBoost: unbiased boosting with categorical features"

ImplementaÃ§Ã£o:
â€¢	Arquivos: src/ml_models/train_xgboost.py, train_lightgbm.py, train_catboost.py
â€¢	OtimizaÃ§Ã£o: Bayesian Optimization
â€¢	ValidaÃ§Ã£o: 5-Fold Cross-Validation



2.3 Modelos White Box
ReferÃªncias:
â€¢	RegressÃ£o LogÃ­stica: [Cox, 1958][7] - "The Regression Analysis of Binary Sequences"
â€¢	Ãrvore de DecisÃ£o: [Quinlan, 1986][8] - "Induction of Decision Trees"

ImplementaÃ§Ã£o:
â€¢	Arquivos: src/ml_models/train_logistic_regression_white_box.py, train_decision_tree_white_box.py
â€¢	Objetivo: ComparaÃ§Ã£o white box vs black box



3. DEEP REINFORCEMENT LEARNING
3.1 Deep Q-Network (DQN)
ReferÃªncia: [Mnih et al., 2015][9] - "Human-level control through deep reinforcement learning"

ImplementaÃ§Ã£o:
â€¢	Arquivo: src/drl/train_dqn.py
â€¢	Arquitetura: 2 Q-networks (principal e alvo)
â€¢	Experience Replay: buffer_size=10000
â€¢	Target Update: Ï„=0.001

EquaÃ§Ã£o (Algoritmo 3 - Tese):
Q(s,a) â† Q(s,a) + Î±[r + Î³ max_a' Q(s',a') - Q(s,a)]



3.2 Proximal Policy Optimization (PPO)
ReferÃªncia: [Schulman et al., 2017][10] - "Proximal Policy Optimization Algorithms"

ImplementaÃ§Ã£o:
â€¢	Arquivo: src/drl/train_ppo.py
â€¢	Arquitetura: Actor-Critic
â€¢	Clipped Surrogate Objective
â€¢	GAE (Generalized Advantage Estimation)

EquaÃ§Ã£o (Algoritmo 5 - Tese):
L^CLIP(Î¸) = ÃŠ_t[min(r_t(Î¸)Ã‚_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)Ã‚_t)]



3.3 Soft Actor-Critic (SAC)
ReferÃªncia: [Haarnoja et al., 2018][11] - "Soft Actor-Critic: Off-Policy Deep Reinforcement Learning with a Stochastic Actor"

ImplementaÃ§Ã£o:
â€¢	Arquivo: src/drl/train_sac.py
â€¢	Arquitetura: Actor + 2 Q-networks (crÃ­ticos duplos)
â€¢	Entropy Regularization: Î± adaptativo
â€¢	Target Networks: soft update

EquaÃ§Ã£o (Algoritmo 6 - Tese):
J(Ï€) = E_s~D[E_a~Ï€[Q(s,a) - Î± log Ï€(a|s)]]



4. NATURAL LANGUAGE PROCESSING
4.1 BioBERT
ReferÃªncia: [Lee et al., 2020][12] - "BioBERT: a pre-trained biomedical language representation model for biomedical text mining"

ImplementaÃ§Ã£o:
â€¢	Arquivo: src/nlp/biobert_model.py
â€¢	Modelo: BioBERT (prÃ©-treinado)
â€¢	Dimensionalidade: 768 (embeddings contextualizados)
â€¢	ReduÃ§Ã£o: PCA, t-SNE, UMAP (opcional)

CaracterÃ­sticas:
â€¢	ExtraÃ§Ã£o de embeddings contextualizados
â€¢	ExtraÃ§Ã£o de entidades clÃ­nicas
â€¢	Modo simulado (quando PyTorch nÃ£o disponÃ­vel)

CÃ³digo:
from src.nlp.biobert_model import train_biobert_pipeline
 
embeddings, metadata = train_biobert_pipeline(
    texts=narrativas,
    reduce_dim=True,
    n_components=50
)



4.2 Narrativas SintÃ©ticas Melhoradas
ReferÃªncia: [Bowman et al., 2015][13] - "Generating Sequences With Recurrent Neural Networks"

ImplementaÃ§Ã£o:
â€¢	Arquivo: src/nlp/synthetic_narratives_improved.py
â€¢	GeraÃ§Ã£o: DeterminÃ­stica (seed=42)
â€¢	VariaÃ§Ã£o linguÃ­stica: Templates contextualizados
â€¢	Sem ruÃ­do aleatÃ³rio desnecessÃ¡rio

CaracterÃ­sticas:
â€¢	VariaÃ§Ã£o linguÃ­stica realista
â€¢	Contexto clÃ­nico complexo
â€¢	Suporte para dados reais



4.3 TF-IDF e LDA
ReferÃªncias:
â€¢	TF-IDF: [Salton & McGill, 1983][14] - "Introduction to Modern Information Retrieval"
â€¢	LDA: [Blei et al., 2003][15] - "Latent Dirichlet Allocation"

ImplementaÃ§Ã£o:
â€¢	Arquivo: src/nlp/text_feature_extraction.py
â€¢	TF-IDF: sklearn.feature_extraction.text.TfidfVectorizer
â€¢	LDA: sklearn.decomposition.LatentDirichletAllocation (n_topics=10)



5. ENSEMBLE
5.1 Ensemble Ponderado com 3 Paradigmas
ReferÃªncia: [Zhou, 2012][16] - "Ensemble Methods: Foundations and Algorithms"

ImplementaÃ§Ã£o:
â€¢	Arquivo: src/ensemble/weighted_ensemble_3_paradigmas.py
â€¢	Pesos:
â—¦	ML: 0.50
â—¦	DRL: 0.30
â—¦	NLP: 0.20

EquaÃ§Ã£o (EquaÃ§Ã£o 81 - Tese, corrigida):
Å·_ensemble(x) = 0.50Â·Å·_ML(x) + 0.30Â·Å·_DRL(x) + 0.20Â·Å·_NLP(x)

ObservaÃ§Ã£o: XAI nÃ£o entra no cÃ¡lculo (erro conceitual na tese original).



6. QUANTIFICAÃ‡ÃƒO DE INCERTEZA
6.1 Monte Carlo Dropout
ReferÃªncia: [Gal & Ghahramani, 2016][17] - "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"

ImplementaÃ§Ã£o:
â€¢	Arquivo: src/ensemble/uncertainty_quantification.py
â€¢	MÃ©todo: T=100 forward passes com dropout

EquaÃ§Ã£o 82 (Tese):
pÌ‚_MC(x) = (1/T) Î£ pÌ‚_t(x)
U_MC(x) = âˆš((1/T) Î£ (pÌ‚_t(x) - pÌ‚_MC(x))Â²)



6.2 VariÃ¢ncia do Ensemble
EquaÃ§Ã£o 83 (Tese, corrigida para 3 paradigmas):
U_ens(x) = âˆš((1/3) Î£ (pÌ‚_i(x) - pÌ‚_ensemble(x))Â²)



6.3 Incerteza Total
EquaÃ§Ã£o 84 (Tese):
U(x) = 0.6Â·U_MC(x) + 0.4Â·U_ens(x)



7. EXPLAINABLE AI (XAI)
7.1 SHAP (SHapley Additive exPlanations)
ReferÃªncia: [Lundberg & Lee, 2017][18] - "A Unified Approach to Interpreting Model Predictions"

ImplementaÃ§Ã£o:
â€¢	Arquivo: src/xai/shap_explainer.py
â€¢	MÃ©todo: TreeExplainer (para modelos baseados em Ã¡rvores)
â€¢	VisualizaÃ§Ãµes: SHAP values, dependence plots, force plots

CÃ³digo:
from src.xai.shap_explainer import ShapExplainer
 
explainer = ShapExplainer(model)
shap_values = explainer.explain(X_test)



7.2 LIME (Local Interpretable Model-Agnostic Explanations)
ReferÃªncia: [Ribeiro et al., 2016][19] - "Why Should I Trust You?: Explaining the Predictions of Any Classifier"

ImplementaÃ§Ã£o:
â€¢	Arquivo: src/xai/lime_explainer.py
â€¢	MÃ©todo: RegressÃ£o local ponderada
â€¢	PerturbaÃ§Ãµes: 5000 amostras
â€¢	Features: K=10

CÃ³digo:
from src.xai.lime_explainer import LimeExplainer
 
explainer = LimeExplainer(model, X_train)
explanation = explainer.explain_instance(x_test)



7.3 MÃ©tricas de Interpretabilidade
ImplementaÃ§Ã£o:
â€¢	Arquivo: src/xai/interpretability_metrics.py
â€¢	MÃ©tricas:
â—¦	Fidelidade (EquaÃ§Ã£o 58 - Tese)
â—¦	Cobertura de features
â—¦	Estabilidade de explicaÃ§Ãµes



8. AVALIAÃ‡ÃƒO
8.1 MÃ©tricas BÃ¡sicas
ImplementaÃ§Ã£o:
â€¢	Arquivo: src/evaluation/metrics.py

EquaÃ§Ã£o 85 - F1-Score:
F1 = 2Â·TP / (2Â·TP + FP + FN)

EquaÃ§Ã£o 86 - AUC-ROC:
AUC = P(Å·(x+) > Å·(x-))



8.2 MÃ©tricas AvanÃ§adas
ImplementaÃ§Ã£o:
â€¢	Arquivo: src/evaluation/advanced_metrics.py

EquaÃ§Ã£o 87 - MCC (Matthews Correlation Coefficient):
MCC = (TPÂ·TN - FPÂ·FN) / âˆš((TP+FP)(TP+FN)(TN+FP)(TN+FN))

EquaÃ§Ã£o 88 - Teste de McNemar:
Ï‡Â² = (b - c)Â² / (b + c) ~ Ï‡Â²(1)

EquaÃ§Ã£o 89 - Intervalos de ConfianÃ§a Bootstrap:
IC = [Î¸_2.5%, Î¸_97.5%]



ğŸ“ Estrutura do Projeto
tb_framework_FINAL/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ tuberculosis-data-06-16.csv          # Dataset TB-WEB-SP
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ data_loader.py                   # Carregamento de dados
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â”œâ”€â”€ missing_values.py                # MICE + Moda
â”‚   â”‚   â”œâ”€â”€ outliers_treatment.py            # Isolation Forest
â”‚   â”‚   â”œâ”€â”€ categorical_encoding.py          # One-Hot + Label
â”‚   â”‚   â”œâ”€â”€ normalization.py                 # NormalizaÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ correlation_treatment.py         # VIF
â”‚   â”‚   â”œâ”€â”€ class_balancing.py               # SMOTE
â”‚   â”‚   â””â”€â”€ preprocessing_pipeline_corrected.py  # Pipeline completo
â”‚   â”œâ”€â”€ ml_models/
â”‚   â”‚   â”œâ”€â”€ train_random_forest.py           # Random Forest
â”‚   â”‚   â”œâ”€â”€ train_xgboost.py                 # XGBoost
â”‚   â”‚   â”œâ”€â”€ train_lightgbm.py                # LightGBM
â”‚   â”‚   â”œâ”€â”€ train_catboost.py                # CatBoost
â”‚   â”‚   â”œâ”€â”€ train_logistic_regression_white_box.py  # LogÃ­stica
â”‚   â”‚   â”œâ”€â”€ train_decision_tree_white_box.py       # Ãrvore
â”‚   â”‚   â””â”€â”€ ml_pipeline.py                   # Pipeline ML
â”‚   â”œâ”€â”€ drl/
â”‚   â”‚   â”œâ”€â”€ environment.py                   # Ambiente RL
â”‚   â”‚   â”œâ”€â”€ train_dqn.py                     # DQN
â”‚   â”‚   â”œâ”€â”€ train_ppo.py                     # PPO
â”‚   â”‚   â”œâ”€â”€ train_sac.py                     # SAC
â”‚   â”‚   â””â”€â”€ drl_pipeline.py                  # Pipeline DRL
â”‚   â”œâ”€â”€ nlp/
â”‚   â”‚   â”œâ”€â”€ biobert_model.py                 # BioBERT
â”‚   â”‚   â”œâ”€â”€ synthetic_narratives_improved.py # Narrativas
â”‚   â”‚   â”œâ”€â”€ text_feature_extraction.py       # TF-IDF + LDA
â”‚   â”‚   â””â”€â”€ nlp_pipeline.py                  # Pipeline NLP
â”‚   â”œâ”€â”€ ensemble/
â”‚   â”‚   â”œâ”€â”€ weighted_ensemble_3_paradigmas.py    # Ensemble
â”‚   â”‚   â””â”€â”€ uncertainty_quantification.py        # Incerteza
â”‚   â”œâ”€â”€ xai/
â”‚   â”‚   â”œâ”€â”€ shap_explainer.py                # SHAP
â”‚   â”‚   â”œâ”€â”€ lime_explainer.py                # LIME
â”‚   â”‚   â””â”€â”€ interpretability_metrics.py      # MÃ©tricas
â”‚   â””â”€â”€ evaluation/
â”‚       â”œâ”€â”€ metrics.py                       # MÃ©tricas bÃ¡sicas
â”‚       â”œâ”€â”€ advanced_metrics.py              # MCC, McNemar, Bootstrap
â”‚       â””â”€â”€ visualizations.py                # VisualizaÃ§Ãµes
â”œâ”€â”€ run_complete_framework.py                # Script completo
â”œâ”€â”€ run_ml_comparison.py                     # ComparaÃ§Ã£o ML
â””â”€â”€ run_xai_and_ensemble.py                  # XAI + Ensemble



ğŸš€ InstalaÃ§Ã£o e Uso
InstalaÃ§Ã£o
# Clonar repositÃ³rio
git clone https://github.com/seu-usuario/tb-framework.git
cd tb-framework
 
# Criar ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
 
# Instalar dependÃªncias
pip install -r requirements.txt

Uso
# Executar pipeline completo
python run_complete_framework.py
 
# ComparaÃ§Ã£o de modelos ML
python run_ml_comparison.py
 
# XAI e Ensemble
python run_xai_and_ensemble.py



ğŸ“š ReferÃªncias AcadÃªmicas
PrÃ©-processamento
[1] Azur, M. J., Stuart, E. A., Frangakis, C., & Leaf, P. J. (2011). "Multiple Imputation by Chained Equations: What is it and how does it work?" International Journal of Methods in Psychiatric Research, 20(1), 40-49.
â€¢	DOI: 10.1002/mpr.329
â€¢	CitaÃ§Ãµes: 4387

[2] Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002). "SMOTE: Synthetic Minority Over-sampling Technique." Journal of Artificial Intelligence Research, 16, 321-357.
â€¢	DOI: 10.1613/jair.953
â€¢	CitaÃ§Ãµes: 41973

Machine Learning
[3] Breiman, L. (2001). "Random Forests." Machine Learning, 45(1), 5-32.
â€¢	DOI: 10.1023/A:1010933404324
â€¢	CitaÃ§Ãµes: 42000+

[4] Chen, T., & Guestrin, C. (2016). "XGBoost: A Scalable Tree Boosting System." Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 785-794.
â€¢	DOI: 10.1145/2939672.2939785
â€¢	CitaÃ§Ãµes: 20000+

[5] Ke, G., Meng, Q., Finley, T., et al. (2017). "LightGBM: A Fast, Distributed, High Performance Gradient Boosting Framework." Advances in Neural Information Processing Systems, 3146-3154.
â€¢	CitaÃ§Ãµes: 8000+

[6] Prokhorenkova, L., Gusev, G., Vorobev, A., et al. (2018). "CatBoost: unbiased boosting with categorical features." Advances in Neural Information Processing Systems, 6639-6649.
â€¢	CitaÃ§Ãµes: 3000+

RegressÃ£o LogÃ­stica e Ãrvores de DecisÃ£o
[7] Cox, D. R. (1958). "The Regression Analysis of Binary Sequences." Journal of the Royal Statistical Society, 20(2), 215-242.
â€¢	CitaÃ§Ãµes: 50000+

[8] Quinlan, J. R. (1986). "Induction of Decision Trees." Machine Learning, 1(1), 81-106.
â€¢	DOI: 10.1023/A:1022604100745
â€¢	CitaÃ§Ãµes: 30000+

Deep Reinforcement Learning
[9] Mnih, V., Kavukcuoglu, K., Silver, D., et al. (2015). "Human-level control through deep reinforcement learning." Nature, 529(7587), 529-533.
â€¢	DOI: 10.1038/nature16961
â€¢	CitaÃ§Ãµes: 15000+

[10] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). "Proximal Policy Optimization Algorithms." arXiv preprint arXiv:1707.06347.
â€¢	CitaÃ§Ãµes: 10000+

[11] Haarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018). "Soft Actor-Critic: Off-Policy Deep Reinforcement Learning with a Stochastic Actor." International Conference on Machine Learning, 1861-1870.
â€¢	CitaÃ§Ãµes: 5000+

Natural Language Processing
[12] Lee, J., Yoon, W., Kim, S., et al. (2020). "BioBERT: a pre-trained biomedical language representation model for biomedical text mining." Bioinformatics, 36(4), 1234-1240.
â€¢	DOI: 10.1093/bioinformatics/btz682
â€¢	CitaÃ§Ãµes: 2000+

[13] Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefowicz, R., & Bengio, S. (2015). "Generating Sequences With Recurrent Neural Networks." arXiv preprint arXiv:1511.06732.
â€¢	CitaÃ§Ãµes: 3000+

[14] Salton, G., & McGill, M. J. (1983). "Introduction to Modern Information Retrieval." McGraw-Hill.
â€¢	CitaÃ§Ãµes: 50000+

[15] Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). "Latent Dirichlet Allocation." Journal of Machine Learning Research, 3, 993-1022.
â€¢	CitaÃ§Ãµes: 30000+

Ensemble Methods
[16] Zhou, Z. H. (2012). "Ensemble Methods: Foundations and Algorithms." CRC Press.
â€¢	CitaÃ§Ãµes: 5000+

Explainable AI
[17] Gal, Y., & Ghahramani, Z. (2016). "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning." International Conference on Machine Learning, 1050-1059.
â€¢	CitaÃ§Ãµes: 5000+

[18] Lundberg, S. M., & Lee, S. I. (2017). "A Unified Approach to Interpreting Model Predictions." Advances in Neural Information Processing Systems, 4765-4774.
â€¢	DOI: 10.48550/arXiv.1705.07874
â€¢	CitaÃ§Ãµes: 49599

[19] Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). "Why Should I Trust You?: Explaining the Predictions of Any Classifier." Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1135-1144.
â€¢	DOI: 10.1145/2939672.2939778
â€¢	CitaÃ§Ãµes: 30177

Tuberculose e AplicaÃ§Ãµes ClÃ­nicas
[20] Vinnard, C., Macintyre, A., Goswami, B., et al. (2013). "First Use of Multiple Imputation with the National Tuberculosis Surveillance System." International Journal of Tuberculosis and Lung Disease, 17(8), 1042-1048.
â€¢	DOI: 10.5588/ijtld.12.0837
â€¢	CitaÃ§Ãµes: 6

[21] Ma, J., Yin, H., Hao, X., Sha, W., et al. (2021). "Development of a random forest model to classify sarcoidosis and tuberculosis." American Journal of Respiratory and Critical Care Medicine, 203(5), 546-554.
â€¢	DOI: 10.1164/rccm.202007-2809OC
â€¢	CitaÃ§Ãµes: 17

[22] Mbona, S. V., Mwambi, H., et al. (2023). "Multiple imputation using chained equations for missing data in survival models: applied to multidrug-resistant tuberculosis and HIV data." Journal of Public Health in Africa, 14(2), 1-12.
â€¢	DOI: 10.4081/jpha.2023.2289
â€¢	CitaÃ§Ãµes: 7

<img width="451" height="649" alt="image" src="https://github.com/user-attachments/assets/66702a35-cd9f-4ff4-b4e6-49d8ff5bdedc" />
